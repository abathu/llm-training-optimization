inference:
  quantization: int8   # 支持 int8/fp16/fp32
  use_tensorrt: false
  max_length: 128
  temperature: 0.7
  top_k: 50