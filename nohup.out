[2025-09-16 17:21:26,095] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:21:27,347] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
W0916 17:21:27.842000 70210 site-packages/torch/distributed/run.py:766] 
W0916 17:21:27.842000 70210 site-packages/torch/distributed/run.py:766] *****************************************
W0916 17:21:27.842000 70210 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 17:21:27.842000 70210 site-packages/torch/distributed/run.py:766] *****************************************
No output directory specified, defaulting to 'trainer_output'. To change this behavior, specify --output_dir when creating TrainingArguments.
PyTorch: setting up devices
No output directory specified, defaulting to 'trainer_output'. To change this behavior, specify --output_dir when creating TrainingArguments.
PyTorch: setting up devices
[2025-09-16 17:21:32,282] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:21:32,301] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:21:33,538] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 17:21:33,550] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 17:21:33,570] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 17:21:33,582] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 17:21:33,582] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/llm-training-optimization/train.py", line 423, in <module>
[rank1]:     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
[rank1]:   File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/hf_argparser.py", line 367, in parse_args_into_dataclasses
[rank1]:     raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
[rank1]: ValueError: Some specified arguments are not used by the HfArgumentParser: [' ']
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/llm-training-optimization/train.py", line 423, in <module>
[rank0]:     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
[rank0]:   File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/hf_argparser.py", line 367, in parse_args_into_dataclasses
[rank0]:     raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
[rank0]: ValueError: Some specified arguments are not used by the HfArgumentParser: [' ']
[rank0]:[W916 17:21:37.383201497 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0916 17:21:38.494000 70210 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 70463) of binary: /home/ubuntu/miniconda3/envs/llm/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/llm/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-16_17:21:38
  host      : radiant-heisenberg
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 70464)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_17:21:38
  host      : radiant-heisenberg
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 70463)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2025-09-16 17:22:43,363] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:22:44,595] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
W0916 17:22:45.056000 70820 site-packages/torch/distributed/run.py:766] 
W0916 17:22:45.056000 70820 site-packages/torch/distributed/run.py:766] *****************************************
W0916 17:22:45.056000 70820 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 17:22:45.056000 70820 site-packages/torch/distributed/run.py:766] *****************************************
No output directory specified, defaulting to 'trainer_output'. To change this behavior, specify --output_dir when creating TrainingArguments.
PyTorch: setting up devices
No output directory specified, defaulting to 'trainer_output'. To change this behavior, specify --output_dir when creating TrainingArguments.
PyTorch: setting up devices
[2025-09-16 17:22:49,360] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:22:49,454] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 17:22:50,634] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 17:22:50,648] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 17:22:50,648] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-16 17:22:50,929] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 17:22:50,948] [INFO] [comm.py:821:init_distributed] cdb=None
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/llm-training-optimization/train.py", line 423, in <module>
[rank1]:     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
[rank1]:   File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/hf_argparser.py", line 367, in parse_args_into_dataclasses
[rank1]:     raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
[rank1]: ValueError: Some specified arguments are not used by the HfArgumentParser: [' ']
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/llm-training-optimization/train.py", line 423, in <module>
[rank0]:     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
[rank0]:   File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/hf_argparser.py", line 367, in parse_args_into_dataclasses
[rank0]:     raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
[rank0]: ValueError: Some specified arguments are not used by the HfArgumentParser: [' ']
[rank0]:[W916 17:22:54.960503271 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0916 17:22:56.205000 70820 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 71053) of binary: /home/ubuntu/miniconda3/envs/llm/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/llm/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-16_17:22:56
  host      : radiant-heisenberg
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 71054)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_17:22:56
  host      : radiant-heisenberg
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 71053)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
